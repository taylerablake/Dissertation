
\chapter{The Cholesky Decomposition: an unconstrained parameterization}
\label{ch2.CholDecomposition}


For any positive-definite matrix covariance matrix, $\Sigma$, we have the following unique decomposition:

\begin{equation}
   T^\prime \Sigma T  = D
   \label{eq:MCD}
   \end{equation}
  \noindent 
where $T$ is a lower triangular matrix with diagonal entries all equal to $1$, and $D$ is a diagonal matrix with positive diagonal entries. The lower triangular entries of $T$ are unconstrained, and when the data have a natural ordering (as in time or space), they have a meaningful statistical interpretation. 

Blah blah blah

Blah Blah Blah

\section{An autoregressive model for elements of the covariance}

Pourahmadi (1999) proposed modeling

   \begin{equation}   
   y_t = \sum_{j=1}^{t-1} \phi_{tj} y_j + \sigma_t \epsilon_t \;\;\;\; t=1,\dots, p 
   \label{eq:Pourahmadimodel} 
   \end{equation}
   \noindent
where $\hat{y}_t$ is the least squares predictor of $y_t$ based on its predecessors, and $\lbrace \epsilon_t =  {y}_t - \hat{y}_t \rbrace$ are uncorrelated with $Cov\left(\epsilon\right) = D = diag\left( \sigma_1^2, \sigma_2^2, \dots, \sigma_p^2 \right)$, $\epsilon = \left(\epsilon_1, \epsilon_2, \dots, \epsilon_p \right)^T$. Then we can write

\[
\epsilon = TY\;\;\;\;\;\;\;\;\;\; T= \left\{ \begin{array}{cr}
-\phi_{ij} & i > j \\
1 & i = j \\
0 & i < j \\
\end{array} \right.
\]
and it follows that  

\[
Cov\left(\epsilon\right)  = T Cov\left( Y \right) T^T = T \Sigma T^T = D
\]

We refer to the $\lbrace \phi_{tj} \rbrace$ and the $\lbrace \sigma_t^2 \rbrace$ as the generalized autoregressive parameters (GARPs) and the generalized innovation variances (GIVs), respectively. 

\section{Covariance estimation as bivariate function estimation}
 
\subsection{Likelihood specification for $\phi$ and $\sigma^2$} 
 
 Blah 
 
 Blah 
 
 Blah
 
\subsection{Expressing the GARPs and IVs as functions}
 
Rather than a vector of longitudinal data points, view $Y = \left(y_{t_1}, \dots, y_{t_n}\right)^T$ and $\epsilon = \left(\epsilon_{t_1}, \dots, \epsilon_{t_n}\right)^T$, ${t_1} \le \dots \le {t_n}$, as a discretizations of the stochastic processes $y\left(t\right)$ and $e\left(s\right) \sim \mathcal{WN}\left(0,1\right)$.  We view $\phi_{ij}$ and $\sigma_i^2$ as values of the smooth function $\phi\left(s,t\right)$, $0 \le s < t \le 1$ and $S\left(t\right) = \log \sigma^2\left(t \right)$, $0 \le t \le 1$ evaluated at the design points. Consequently, we view the entries of $\Sigma$ as values of a smooth covariance function, $\gamma\left(s,t\right)$, evaluated at the distinct pairs of design points. We define $y$ as a stochastic process as follows:

 \begin{equation}
 y\left(s\right) = \int_0^s \phi\left(s,t\right)y\left(t\right)dt + \sigma\left(s\right)e\left(s\right)
 \end{equation}
 
Re-expressing \eqref{eq:Pourahmadimodel} in terms of $\phi\left(s,t\right)$ and $\sigma^2\left(t\right)$, we have

   \begin{equation}   
   \hat{y}\left(t_i\right)  = \sum_{j=1}^{i-1} \phi\left(t_i ,t_j\right) y\left({t_j}\right) + \sigma\left(t_i\right)\epsilon\left({t_i}\right) \;\;\;\; i=1,\dots, n 
   \label{eq:MyModel} 
   \end{equation}

We first turn our focus to the estimation of $\phi\left(s,t\right)$ and assume that $\sigma^2\left(t\right)$ is known, and WLOG, let $\sigma^2\left(t\right) = 1$ for all $t \in \left[0,1\right]$. It is well known that the sample covariance matrix is unstable when the dimensionality of the data is high, so the attraction of regularization is obvious. 

\subsection{Reparameterizing the Generalized Autoregressive Coefficient Function}
Rather than imposing structure on the unconstrained values of $\phi\left(s,t \right)$, we instead consider a rotation of the axes of the input space and re-express $\phi$ in terms of the transformed coordinates $\begin{bmatrix} l\\ m	
\end{bmatrix} = \begin{bmatrix} s - t\\	 \frac{\left(s + t\right)}{2}\end{bmatrix}$

\begin{eqnarray*}
\phi\left(s,t\right) &=& \phi^*\left(s-t,\frac{s+t}{2}\right) \\
 &=& \phi^*\left(l,m\right)\\
\end{eqnarray*}

Imposing structure on $\phi^*$ naturally leads to null models presented in earlier work on covariance estimation with appropriate choice of penalty in the smoothing spline problem formulation. 
    
    
Several approaches to estimating the covariance parameters have been suggested, the simplest of which is the usual maximum likelihood estimation assuming a Normal distribution for $\epsilon$. Garcia, Kohli, and Pourahmadi (2001) make the heuristic argument that, for fixed $t$, $\phi_{t,t-l}$ should be small for large values of $l$, assuming that the linear relationship between $y_t$ and $y_{t-l}$ diminishes as lag increases. They enforce $\phi_{t,t-l}$ to be monotonically decreasing in $l$. 
%% Need to include more shit here about shrinkage 
To model $\phi$, they examine empirical regressograms: plots of $\hat{\phi}_{t,t-j}$ against lag, $j$ and use these to choose a parametric form for $\hat{\phi}\left(t,t-j\right)$. Similarly, they produce innovariograms, plots of the log estimated innovation variances $\hat{\sigma}_t^2$ versus $t$, and select an appropriate functional form. They choose to model the innovation variance function as polynomial function of $t$, so that model selection is equivalent to choosing the appropriate degree of polynomial, using ordinary least squares to fit model coefficients. Note that this approach 

%% Go on about the nested lasso penalty for banding 

Chen et. al. presented a semiparametric approach for simultaneously estimating a mean function and covariance structure, modeling $\phi\left(s,t\right)$ as a polynomial in $s-t$. By specifying the generalized autoregressive coefficient function to be a function of lag alone, they make the implicit assumption that the underlying stochastic process is stationary. This dramatically aids in dimension reduction, as the number of distinct entries in the covariance matrix decreases from $p\left(p-1\right)/2$ to $p$ as a result. 

%Stuff about banding the covariance matrix and its relation to antedependence models of order, say, k...
Huang et. al (2006) estimate the elements of $T$ via normal maximum likelihood with a Lasso penalty, which introduces sparsity in $T$ with zeros but in arbitrarily placed entries. Using the Cholesky decomposition to ensure positive-definiteness, Wu and Pourahmadi (2003) presented a $k-diagonal$ estimator which bands the inverse of the covariance matrix by smoothing down the first $k$ diagonals of $T$ and setting the rest to zero, choosing the number of nonzero diagonals by AIC using a normal likelihood. Huang et. al. use spline functions to smooth $\sigma_t^2\rbrace_{t=1}^p$ and $\lbrace \phi_{t,t-j} \rbrace_{j=1}^{t-1}$, the sub-diagonals of $T$ which hold the lag-$J$ regression coefficients and are closely related to time-varying autoregressive models. They use penalized maximum likelihood to estimate the spline coefficients, treating the $p-1$ sub-diagonals as $p-1$ separate smoothing spline problems. While computationally  does not make use of the potential information about the dependence structure lying in the direction orthogonal to the diagonal; we propose using bivariate smoothing to utilize information in both directions.


\subsection{The Multicenter AIDS Cohort Study and a problem notation illustration}

CD4 cells counts are a commonly used marker in the progression of AIDS, as they are an integral assessment of immune system status. Data from the Multicenter AIDS Cohort Study provides data collected on 283 homosexual males who were infected between 1984 and 1991. Repeated measurements included CD4 cell percentages (CD4 cell counts divided by the total number of lymphocytes, a certain type of white blood cells.) All subjects were scheduled to have measurements taken at semi-annual visits, but there are different numbers of observations per individual and different observation times $t_{ij}$ for each subject due to many subjects missing appointments and the fact that infections arose at different times within the study period.  Denote the vectors of measurements on individual $i$ by $Y_i = \left( y\left(t_{i1}\right), \dots, y\left(t_{i n_i}\right) \right)^T$ with corresponding measurement times  $T_i = \left(t_{i1},\dots, t_{i n_i}  \right)^T$, $0 \le t_{ij} < t_{ik} \le 1$ for $j < k$. A total of $N_Y = \sum_{i=1}^N n_i = 2376$ measurements were taken on $N=283$ subjects, with the number of within-subject measurements, $\lbrace n_i \rbrace$, ranging from 1 to 14.  Denote the set of unique observations times by $ \mathcal{T} = \bigcup_{i=1}^N \bigcup_{j=1}^{n_i} \lbrace t_{ij} \rbrace$ and the number of unique observation times by $\cardT = N_T$. Relabel the elements so that $\mathcal{T} = \lbrace t_1, \dots, t_{N_T} \rbrace$ and define

\[
Y = \left(Y\left(t_1\right) , \dots, Y\left(t_{N_T} \right)\right)^\prime\;\;\;\;\;\;\;\; Cov\left(Y\right) = \Sigma_{N_T \times N_T}
\]
\noindent
where the $ij$th entry of $\Sigma$ is defined to be $\Sigma_{ij} = Cov\left(y\left(t_i\right), y\left(t_j\right)  \right) = \gamma\left(t_i,t_j\right)$. In other words, we view the entries of $\Sigma$ as the value of a smooth bivariate function $\gamma$ at $\lbrace \left(t_i,t_j\right) \rbrace$. Writing \eqref{eq:MyModel} in terms of the transformed coordinates, $\left(l_{ij}, m_{ij}\right)$, the autoregressive model arising from the Cholesky decomposition of $\Sigma$ becomes

\begin{eqnarray}   
\hat{y}\left(t_i\right)  &=& \sum_{j=1}^{i-1} \phi\left(t_i ,t_j\right) y\left({t_j}\right) + \epsilon\left({t_i}\right)\nonumber \\ 
&=& \sum_{j=1}^{i-1} \phi^*\left(t_i - t_j, \frac{1}{2}\left(t_i - t_j\right)\right) y\left({t_j}\right) + \epsilon\left({t_i}\right) \nonumber \\
&=& \sum_{j=1}^{i-1} \phi^*\left(l_{ij}, m_{ij}\right)  y\left({t_j}\right) + \epsilon\left({t_i}\right) \label{eq:MyTransformedModel}
\end{eqnarray}  
\noindent  
where $\phi^*:\left(0,1\right)\times \left(0,1\right)\rightarrow \mathrm{R}^+$ is a smooth bivariate function, and the transformed design points have been scaled so that $l_{ij},m_{ij} \in \left(0,1\right)$.

\section{A Structured Family of Nonparametric Models for the Covariance Structure}


Let $\mathcal{H} = \mathcal{H}_{0} \oplus \mathcal{H}_{1}$ be the reproducing kernel Hilbert space (r.k.h.s) corresponding to the tensor product of the first-order and second-order Sobolev spaces:

\[
\mathcal{H} = \mathcal{H}_{l} \otimes \mathcal{H}_{m}, \;\; \mathcal{H}_{l} = W_2\left(0,1\right),\;\;\mathcal{H}_{m} = W_1\left(0,1\right)\;\mbox{where }
\]

\[W_m\left(0,1\right) \equiv \lbrace f: \;\;f^\prime, \dots, f^{\left( m-1 \right)} \mbox{absolutely continuous}, \int_0^1 \left(f^{\left( m \right)}\right)^2 dt < \infty \rbrace\]
\noindent
We seek $\phi^*\left(\cdot, \cdot \right) \in \mathcal{H}$ which minimizes
\begin{equation}
\frac{1}{2}\sum_{i=1}^N \sum_{j=2}^{n_i} {\sigma^{-2}_{ij}}\left( y\left(t_{ij}\right) - \sum_{k=1}^{n_i - 1}\phi^*\left(l^i_{jk},m^i_{jk} \right)y\left(t_{ik}\right) \right)^2 + \lambda J\left(\phi^*\right)  
\label{eq:objectivefun}
\end{equation}
\noindent
where $P_1 \phi^*$ is the projection of $\phi^*$ onto $\mathcal{H}_1$, $J\left(\phi^*\right) = \vert \vert P_1 \phi^* \vert \vert^2$. Define the differential operator $M_\nu f = \int_0^1 f^{\left( m \right)}\left(x\right) dx\;,\;\; \nu = 1, \dots, m$ and endow $W_m\left(0,1\right)$ with inner product
%\begin{equation}
%\left< f,g\right> = \underbrace{\sum_{\nu=0}^{m-1} M_\nu f M_\nu g}_{\left< f,g\right>_0} + \underbrace{\int_0^1 f^{\left( m \right)}\left(x\right)g^{\left( m \right)}\left(x\right)dx}_{\left< f,g\right>_1}
%\end{equation}

\begin{equation}
\left< f,g\right> = \left< f,g\right>_0 + \left< f,g\right>_1 = \sum_{\nu=0}^{m-1} M_\nu f M_\nu g + \int_0^1 f^{\left( m \right)}\left(x\right)g^{\left( m \right)}\left(x\right)dx
\end{equation}
\noindent
which induces norm 
\[
\vert \vert f \vert \vert^2 = \left< f,f\right> = \left< f,f\right>_0 + \left< f,f\right>_1 = \vert \vert P_0 f \vert \vert^2 + \vert \vert P_1 f \vert \vert^2
\]
\noindent
Let $k_j\left(x\right) = B_j\left(x\right)/{j!}$ for $x \in \left[0,1\right]$, where $B_j\left(x\right)$ is the $j^{th}$ Bernoulli polynomial which can be defined according to the recursive relationship:

\[
B_0\left(x\right) = 1,\;\;\;\;\;\; \frac{d}{dx} B_r\left(x\right) = rB_{r-1}\left(x\right)
\]
\noindent
Noting that $M_\nu B_r = \delta_{\nu-r}$, $W_m$ can be written as a direct sum of the $m$ orthogonal subspaces: $\lbrace k_r \rbrace_{r=0}^{m-1}$ and $W_m^1$.   Here, $\lbrace k_r \rbrace$ is the subspace spanned by $k_r$ and $W_m^1$ is the space orthogonal to $W_m^0 \equiv \lbrace 1 \rbrace \oplus \lbrace k_1 \rbrace \oplus \dots \oplus \lbrace k_{m-1} \rbrace$ which satisfies 
\[
W_m^1 = \lbrace f: M_\nu f = 0,\;\; \nu = 0,1,\dots, m-1\rbrace
\]

\noindent
Writing $\mathcal{H}$ as the tensor product of the two decomposed Sobolev spaces, we have

\begin{eqnarray}
\mathcal{H} = \mathcal{H}_l  \otimes \mathcal{H}_m &=& W_2 \otimes W_1 \label{eq:HilbertDecomp} \\ 
&=& \left[ W_2^0 \oplus W_2^1 \right] \otimes \left[ W_1^0 \oplus W_1^1 \right] \nonumber \\ 
&=& \left[ \left[ \lbrace 1 \rbrace \oplus \lbrace k_1 \rbrace \right] \oplus W_2^1 \right] \otimes \left[ \lbrace 1 \rbrace \oplus W_1^1 \right] \nonumber \\ 
&=&\left[ \lbrace 1 \rbrace  \oplus \lbrace k_1 \rbrace \right] \oplus W_2^1 \oplus W_1^1 \oplus  \left[ \lbrace k_1 \rbrace  \otimes  W_1^1 \right]  \oplus  \left[W_2^1 \otimes  W_1^1   \right] \nonumber \\
&\equiv& \left[ \mathcal{H}_{\mu^*} \oplus \mathcal{H}_l^0 \right] \oplus \left[ \mathcal{H}_l^1 \oplus \mathcal{H}_m^1 \oplus \mathcal{H}_{lm}^{01} \oplus \mathcal{H}_{lm}^{11}\right]
\nonumber\\
&=& \mathcal{H}_0 \oplus \mathcal{H}_1
\nonumber
\end{eqnarray} 

\noindent
where the functional components corresponding to $\mathcal{H}_\mu$, $\mathcal{H}_l^0$, $\mathcal{H}_l^1$, $\mathcal{H}_m^1$, and $\left[ \mathcal{H}_{lm}^{01} \oplus \mathcal{H}_{lm}^{11}\right]$ are the overall mean, the nonparametric main effect of $l$, the parametric main effect of $l$, the parametric main effect of $m$, the nonparametric-parametric interaction, and the parametric-parametric interaction (between $l$ and $m$). Given this decomposition of the function space, any $\phi^* \in \mathcal{H}$ may be written as a sum of components from each of the 

\begin{equation}
\phi^*\left(l,m\right) = \mu^* + \phi_l^*\left(l\right) + \phi_m^*\left(m\right) + \phi_{lm}^*\left(l,m\right)  \label{eq:ANOVAdecomp}
\end{equation} 
\noindent
where $\int_{0}^1 \phi^*_{l}\left(l\right)dl = \int_{0}^1 \phi^*_{m}\left(m\right)dm = 0$, $\int_{0}^1 \phi^*_{lm}\left(l,m\right)dl = \int_{0}^1 \phi^*_{lm}\left(l,m\right)dm = 0$. The reproducing kernel (r.k.) for $\lbrace k_r \rbrace$ is $k_r\left(x \right)k_r\left(x^\prime \right)$. It can be verified that the r.k. for $W_m^1$ (Craven and Wahba 1979) is given by $R^1\left(x,x^\prime\right) = k_m\left(x \right)k_m\left(x^\prime \right) + \left( -1 \right)^{m-1}k_{2m}\left(\left[ x-x^\prime \right] \right)$
where $\left[ \alpha \right]$ is the fractional part of $\alpha$. The r.k. for $W_m$ is given by 
\begin{eqnarray*}
R\left(x,x^\prime\right) &=& R^0\left(x,x^\prime\right) + R^1\left(x,x^\prime\right) \\
&=&\left[ \sum_{\nu=1}^{m-1} k_\nu\left(x \right)k_\nu\left(x^\prime \right) \right]+ \left[ k_m\left(x \right)k_m\left(x^\prime \right) + \left( -1 \right)^{m-1}k_{2m}\left(\left[ x-x^\prime \right] \right)\right] \label{eq:RKforH1}
\end{eqnarray*}
\noindent
Using the fact that the r.k. for a tensor product space is the product of the corresponding reproducing kernels, the r.k. for $\mathcal{H}$ is given by 
\begin{eqnarray}
R\left( \left(l,m\right),\left(l^\prime,m^\prime\right)\right) &=&  R_l\left(l,l^\prime\right) \times R_m\left(m,m^\prime\right) \nonumber \\
 &=& \left[  R_l^0\left(l,l^\prime\right) + R_l^1\left(l,l^\prime\right) \right] \times \left[  R_m^0\left(l,l^\prime\right) + R_m^1\left(l,l^\prime\right) \right] \nonumber \\
 &=& R_l^0\left(l,l^\prime\right)R_m^0\left(m,m^\prime\right) + R_l^0\left(l,l^\prime\right) R_m^1\left(m,m^\prime\right) \nonumber \\
&\mbox{ }&\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; +  R_l^1\left(l,l^\prime\right) R_m^0\left(m,m^\prime\right)  + R_l^1\left(l,l^\prime\right) R_m^1\left(m,m^\prime\right) \nonumber \\
&=& \left[ k_1\left(l \right)k_1\left(l^\prime \right)\right] + \left[ R_l^1\left(l,l^\prime\right)  + k_1\left(l,l^\prime\right) R_m^1\left(m,m^\prime\right) + R_l^1\left(l,l^\prime\right) R_m^1\left(m,m^\prime\right)\right] \nonumber \\
&=& R^0\left( \left(l,m\right) , \left(l^\prime,m^\prime \right) \right) + R^1\left( \left(l,m\right) , \left(l^\prime,m^\prime \right) \right)
\end{eqnarray}


We must introduce some notation to simplify the following expression of the form of the elements in $\mathcal{H}$. Denote the set of unique pairs of observed within-subject time points and the corresponding set of unique transformed coordinates by $\mathcal{W}$ and $\mathcal{W}^*$, respectively:

\begin{eqnarray*}
\mathcal{W} &=& \bigcup_{i=1}^N \bigcup_{j>k}\left(t_{ij} ,t_{ik} \right)\\
\mathcal{W}^* &=& \bigcup_{i=1}^N \bigcup_{j>k}\left(t_{ij}-t_{ik} ,\frac{1}{2}\left( t_{ij}+t_{ik} \right) \right) = \bigcup_{i=1}^N \bigcup_{j>k}\left(l^i_{jk},m^i_{jk} \right)\\
\end{eqnarray*}
\noindent
with $\vert \mathcal{W}\vert = \vert \mathcal{W}^* \vert = N_{\phi^*}$. For simplicity of presentation, relabel the elements of $\mathcal{W}^*$ so that 
\[
\mathcal{W}^* = \lbrace \left( l_1,m_1 \right), \left( l_2,m_2 \right), \dots, \left( l_{N_{\phi^*}},m_{N_{\phi^*}} \right)  \rbrace
\]
\noindent
Then we may verify that any $\phi^* \in \mathcal{H}$ can be written 
\[
\phi^*\left(l,m \right) = d_0 + d_1k_1\left(l\right) + \sum_{i=1}^n  c_i R_1\left( \left(l,m\right) , \left(l_i,m_i \right)\right) + \rho\left(l,m\right)
\]
\noindent
where $\rho \perp \mathcal{H}_0 = \lbrace 1\rbrace \oplus \lbrace k_1\rbrace,\; span\lbrace R_1\left(\left(l_i, m_i \right),\cdot \right)  \rbrace$. We do so by demonstrating that  $\rho$ does not improve the first term in \eqref{eq:objectivefun} (the data fit functional) and only adds to the penalty term, $J\left(\phi^*\right)$. Consequently, if $\hat{\phi^*}$ is the minimizer of \eqref{eq:objectivefun}, then $\rho = 0$. Using the properties of reproducing kernels, we can rewrite $\phi^*$ as an inner product of itself with $R$:
 
\begin{eqnarray*}
\phi^*\left(l_j,m_j \right)  &=& \left< R\left(\left(l_j,m_j\right),\left(\cdot,\cdot\right) \right),\phi^*\left(\cdot,\cdot\right)\right>\\
&=& \left<R_0\left( \left(l_j,m_j\right),\left(\cdot,\cdot\right) \right) + R_1\left(\left(l_j,m_j\right),\left(\cdot,\cdot\right) \right),d_0 + d_1k_1\left(\cdot \right)\right. \\ 
&\mbox{ }&\left. \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+ \sum_{i=1}^{N_{\phi^*}}  c_i R_1\left( \left(l_i,m_i \right),\left(\cdot,\cdot\right) \right) + \rho\left(\left(\cdot,\cdot \right)\right)\right>\\
&=& \left<R_0\left( \left(l_j,m_j\right),\left(\cdot,\cdot\right) \right) , d_0 + d_1k_1\left(\cdot\right)\right> + \left< R_0\left( \left(l_j,m_j\right),\left(\cdot,\cdot\right) \right),\sum_{i=1}^{N_{\phi^*}}  c_i R_1\left( \left(l_i,m_i \right),\left(\cdot,\cdot\right) \right)\right> \\
&\mbox{ }& + \left<R_0\left( \left(l_j,m_j\right),\left(\cdot,\cdot\right) \right), \rho\left(\left(\cdot,\cdot \right)\right)\right> + \left<R_1\left(\left(l_j,m_j\right),\left(\cdot,\cdot\right) \right), d_0 + d_1k_1\left(\cdot \right)\right> \\
&\mbox{ }& + \left<R_1\left(\left(l_j,m_j\right),\left(\cdot,\cdot\right) \right),\sum_{i=1}^{N_{\phi^*}}  c_i R_1\left( \left(l_i,m_i \right),\left(\cdot,\cdot\right) \right) \right> + \left<R_1\left(\left(l_j,m_j\right),\left(\cdot,\cdot\right) \right), \rho\left(\left(\cdot,\cdot \right)\right)\right>\\
&=& \left<R_0\left( \left(l_j,m_j\right),\left(\cdot,\cdot\right) \right) , d_0 + d_1k_1\left(\cdot\right)\right> + \left<R_1\left(\left(l_j,m_j\right),\left(\cdot,\cdot\right) \right),\sum_{i=1}^{N_{\phi^*}}  c_i R_1\left( \left(l_i,m_i \right),\left(\cdot,\cdot\right) \right) \right> \\
&\mbox{ }& + \underbrace{\left<R_0\left( \left(l_j,m_j\right),\left(\cdot,\cdot\right) \right)  , \rho\left(\cdot,\cdot\right) \right>}_{0} + \underbrace{\left<R_1\left( \left(l_j,m_j\right),\left(\cdot,\cdot\right) \right)  , \rho\left(\cdot,\cdot\right) \right>}_{0}\\
&=& d_0 + d_1k_1\left(\cdot \right) + \sum_{i=1}^{N_{\phi^*}}  c_i R_1\left( \left(l_i,m_i \right),\left(l_j,m_j\right) \right)
\end{eqnarray*}
\noindent


Rewriting the data fit functional, we have that  
 \begin{eqnarray*}
&\mbox{ }&\sum_{i=1}^N \sum_{j=1}^{n_i} \sigma_{ij}^{-2} \left(y\left(t_{ij}\right) - \sum_{k=1}^{j-1} \phi^*\left(t_{ij}, t_{ik}  \right) y\left(t_{ik}\right)  \right)^2  \\ 
&=& \sum_{i=1}^N \sum_{j=1}^{n_i} \sigma_{ij}^{-2} \left(y\left(t_{ij}\right) - \sum_{k=1}^{j-1} \left< R\left(\left(l^i_{jk},m^i_{jk}\right),\left(\cdot,\cdot\right) \right),\phi^*\left(\cdot,\cdot\right)\right> y\left(t_{ik}\right)  \right)^2  \\
 \end{eqnarray*}
\noindent
which is free of $\rho$. Consider the contribution of any nonzero $\rho$ to $J\left(\phi^*\right)$: 
  
 \begin{eqnarray*}
 J\left(\phi^*\right) &=& \vert \vert  P_1\phi^* \vert \vert^2\\
 &=& \left< \sum_{i=1}^{N_{\phi^*}}  c_i R_1\left( \left(l_i,m_i\right),\left(\cdot,\cdot\right) \right) + \rho\left(\cdot,\cdot \right), \sum_{j=1}^{N_{\phi^*}} c_j R_1\left( \left(l_j,m_j\right),\left(\cdot,\cdot\right) \right) + \rho\left(\cdot,\cdot\right)\right> \\
 &=& \vert \vert \sum_{i=1}^{N_{\phi^*}}  c_i R_1\left(\left(l_i,m_i\right),\left(\cdot,\cdot\right) \right) \vert \vert^2 + \vert \vert  \rho \vert \vert^2 
 \end{eqnarray*}
\noindent
Thus, including $\rho$ in $\phi^*$ only increases the penalty without improving (decreasing) the data fit functional, so we indeed have that the minimizer of \eqref{eq:objectivefun} has the form
\begin{equation}
 \phi^*\left(l,m\right) =  d_0 + d_1k_1\left(l\right) + \sum_{i=1}^{N_{\phi^*}} c_i R_1\left( \left(l,m\right) , \left(l_i,m_i \right)\right)
 \label{eq:finitedimsolution}
 \end{equation}
 
 Consider moving along the entries of $\Sigma$ in the $l$ direction toward the diagonal. $T$ was explicitly constructed with unit diagonal so that $\phi_{tt} = \phi^*_{0t} = 1$. Considering this constraint when viewing $\phi^*$ as a continuous function,
 
 \begin{equation}
\lim_{l\to0} \phi^*\left(l,m\right) = 1\;\;for\; all\; m \in \left(0,1\right)
\label{eq:unitconstraint}
 \end{equation}
 \noindent
To obtain solutions satisfying \eqref{eq:unitconstraint}, we isolate the seach the minimizer of \eqref{eq:objectivefun}  to those belonging to the convex subspace of $\mathcal{H}$ given by
 \[
 \mathcal{C} = \lbrace \phi^*: \phi^*\left(0,m\right),\;\;m \in \left(0,1\right)  \rbrace
 \]
\noindent
We approximate the set of functions $\phi^*$ satisfying the infinite set of linear constraints, $\mathcal{C}$, with functions satisfying the finite family of constraints, $\mathcal{C}_{N_m}$:

\[
 \mathcal{C}_{N_m} = \lbrace \phi^*: \phi^*\left(0,m_j \right),\;\;j = 1, \dots, N_m  \rbrace
 \]
\noindent where $N_m$ denotes the total number of unique observed values of $m$: $N_m = \vert \mathcal{M} \vert$, $\mathcal{M} = \bigcup_{i=1}^{N_{\phi^*}} m_i$. Using a similar argument to that presented above, Villalobos and Wahba have shown that if 
\[
L_1, \dots, L_{N_{\phi^*}}, C_1, \dots, C_{N_m}\]
\noindent are linearly independent functionals with $L_i$ being the usual evaluation functional and $C_j$ defines the $j$th linear constraint in that $C_j \phi^* = \phi^*\left(0,m_j\right)$, then the unique minimizer of \eqref{eq:objectivefun} has the form

\begin{equation}
\phi^*\left(l,m\right) =  d_0 + d_1k_1\left(l\right) + \sum_{i=1}^{N_{\phi^*}} c_i R_1\left( \left(l,m\right) , \left(l_i,m_i \right)\right) + \sum_{j=1}^{N_{m}} b_j R_1\left( \left(l,m\right) , \left(0,m_j \right)\right)
\end{equation}

\noindent
Expressing the components in the ANOVA decomposition of $\phi^*$ given by \eqref{eq:ANOVAdecomp} as expansions of $\lbrace 1 \rbrace, \lbrace k_1 \rbrace$, and $\lbrace R_1\left(\left(l_i,m_i \right), \left(\cdot,\cdot\right)\right) = R^l_1\left(l_i,\cdot \right) + R^m_1\left(m_i,\cdot \right) + R^{lm}_1\left(\left(l_i,m_i \right), \left(\cdot,\cdot\right)\right) \rbrace$, we can write

\begin{eqnarray*}
\phi^*\left(l,m\right) &=& \mu^* +  \phi_l^*\left(l\right) + \phi_m^*\left(m\right) + \phi_{lm}^*\left(l,m\right)\\
&=& d_0 + \left[ d_1k_1\left(l\right) + \sum_{i=1}^{N_{\phi^*}}  c_i R^l_1\left( l,l_i\right) + \sum_{j=1}^{N_{m}}  b_j R^l_1\left( l,0\right) \right]\\
&\mbox{ }& \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; +\left[ \sum_{i=1}^{N_{\phi^*}}  c_i R^m_1\left( m,m_i\right) + \sum_{j=1}^{N_{m}}  b_j R^m_1\left( m,m_j\right)\right] \\
&\mbox{ }& \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+\left[  \sum_{i=1}^{N_{\phi^*}} c_i R^{lm}_1\left(\left( l,l_i\right), \left( m,m_i\right)\right) +  \sum_{i=1}^{N_m} b_j R^{lm}_1\left(\left( l,0\right), \left( m,m_j\right)\right) \right]
\end{eqnarray*}
\noindent
With this decomposition of $\phi^*$, it is easy to see that any non-stationarity of $y\left(t\right)$ is captured by the main effect of $m$, $\phi^*_m$ and the $l-m$ interaction, $\phi^*_{lm}$. The function spaces corresponding to $\phi^*_m$ and $\phi^*_{lm}$ belong entirely to $\mathcal{H}_1$, so that any functional component representing non-stationarity is penalized, including those belonging to the span of linear functions. 

\section{Model Fitting via Penalized Maximum Likelihood}



Let $\Phi^*$ be the $N_{\phi^*} \times 1$ vector of regression coefficients given by \eqref{eq:MyTransformedModel} corresponding to $\phi^*$ evaluated at the elements of $\mathcal{W}^*$, $\Phi^* = \left(\phi^*_1,\phi^*_2, \dots, \phi^*_{N_{\phi^*}}  \right)^T$. Let $d = \left(d_0, d_1\right)^T$, $c = \left(c_1, \dots, c_{N_{\phi^*}}  \right)^T$, and $b = \left(b_1, \dots, b_{N_m}  \right)^T$.   Define $K_{11}$, $K_{12}$, $K_{22}$, $B_{1}$, and $B_2$ as follows: 

\[
\begin{array}{ll}
{K_{11}}\left[i,j\right] = R_1\left(\left(l_i,m_i\right),\left(l_j,m_j\right)\right) &  i,j = 1, \dots, N_{\phi^*}\\
{K_{12}}\left[i,j\right] = R_1\left(\left(l_i,m_i\right),\left(0,m_j\right)\right) &  i = 1, \dots, N_{\phi^*},\;j = 1, \dots, N_m \\
{K_{22}}\left[i,j\right] = R_1\left(\left(0,m_i\right),\left(0,m_j\right)\right) &  i,j = 1, \dots, N_m\\
B_1\left[i , j\right] = k_j\left(l_i\right) &  i = 1, \dots, N_{\phi^*},\;j = 1, 2\\
B_2\left[i , j\right] = k_j\left(0\right) &  i = 1, \dots, N_m,\;j = 1, 2\\
\end{array}
\]
\[
K = \left[\begin{array}{cc}
		K_{11} & K_{12}\\
		K_{12}^T & K_{22}
		\end{array}\right] = \left[\begin{array}{c}
		K_1\\
		K_2
		\end{array}\right]\\;\;\;\; B = \left[ \begin{array}{c}B_1\\
										       B_2\end{array}\right] \]


\noindent
In matrix notation:
\[
\Phi^* = B_1 d + K_1a							
\]
\noindent
where $a$ is the $\left( N_{\phi^*}+ N_m \right) \times 1$ vector $a = \left(c^T \; b^T  \right)^T$. Let $Y_{i,\left(-1\right)}$ denote the vector of the last $n_i - 1$ responses for individual $i$: 
\[
Y_{i,\left(-1\right)} = \left(y\left(t_{i2}\right), y\left(t_{i3}\right), \dots, y\left(t_{i,n_i}\right)\right)^T
\]
\noindent
and concatenate these to construct the $\left(n-N\right)\times 1$ vector $Y_{\left(-1\right)} = \left( Y_{1,\left(-1\right)}^T, Y_{2,\left(-1\right)}^T, \dots, Y_{N,\left(-1\right)} \right)^T$ where $n = \sum_{i=1}^N n_i$. Let $D = diag\left(\sigma^2_{12},\dots,\sigma^2_{1,n_1},\dots,\sigma^2_{N2},\dots,\sigma^2_{N,n_N}\right)$. For appropriate specification of $\left(n-N\right) \times N_{\phi^*}$ design matrix, $Z_Y$, our goal is to minimize:

\begin{equation}
Q\left(a , d \right) =  \frac{1}{2}\left(Y_{\left(-1\right)} - Z_Y\left(B_1d + Ka \right) \right)^T D^{-1} \left(Y_{\left(-1\right)} -  Z\left(Bd + Ka \right)\right) + \lambda a^T K c\\ 
\label{eq:matrixobjfun}
\end{equation} 
\noindent
subject to $B_2 d + K_2 a - \mathbf{1} = 0$, where $Q\left(a , d \right)$ is the log-likelihood of $e  = Y_{\left(-1\right)} - \hat{Y}_{\left(-1\right)} \sim \mathcal{N}\left(0,D\right)$ Augmenting the $\left(n-N\right) \times 1$ residual vector $Y_{\left(-1\right)} - Z_Y\left(B_1d + Rc \right)$ with the $N_m \times 1$ vector of zeros corresponding to these constraints leaves the data fit functional in \eqref{eq:matrixobjfun} unchanged. Let 
\begin{eqnarray*}
Z_{\left(n-N\right) \times \left(N_{\phi^*} + N_m\right)} = \left[Z_Y \; \mathrm{I}_{N_m} \right]\\
Y_{aug} = \left( Y_{\left(-1\right)}^T \;\; \mathbf{1}^T \right)^T
\end{eqnarray*}
\noindent
and augment the diagonal entries of $D$ with $\mathbf{1}_{N_m}$. Define
\begin{equation}
Q^*\left(a,d\right) = \left[Y_{aug} - Z\left(Bd + Ka\right) \right]^T D^{-1} \left[Y_{aug} - Z\left(Bd + Ka\right) \right] + \lambda a^T K a
\label{objectivefunaug}
\end{equation}
\noindent
We obtain the following normal equations by differentiation with respect to $a$ and $d$:
\begin{eqnarray}
\frac{\partial Q^*}{\partial a} &=& - \left(ZK\right)^T D^{-1}\left[Y_{aug} - Z\left(Bd + Ka\right) \right] + \lambda Ka= 0 \label{eq:normaleq1} \\
\frac{\partial Q^*}{\partial d} &=& - \left(ZB\right)^T D^{-1}\left[Y_{aug} - Z\left(Bd + Ka\right) \right] = 0 \label{eq:normaleq2} 
\end{eqnarray}

\noindent
Using the QR decomposition of $B$, we may write $B = \left[ Q_1 \; Q_2\right]\left[ \begin{array}{c} R \\ 0 \end{array}\right] = Q_1 R$,  $Q$ othogonal. Premultiplying \eqref{eq:normaleq2} by $K^{-1}$, we have 
\begin{eqnarray}
 \lambda a &=& Z^T D^{-1}\left[Y_{aug} - Z\left(Bd + Ka\right)\right] \label{eq:solnconst1}  \\
\Longrightarrow 0 =  \eqref{eq:normaleq2} &=&  - B^T\left[ Z^TD^{-1}\left(Y_{aug} - Z\left(Bd + Ka\right) \right) \right] \nonumber\\
 &=& -\lambda B^T a \nonumber \\
 \Longrightarrow a &=& Q_2 e\;for\; e \in \mathcal{R}^{N_{\phi^*} + N_m - 2} \nonumber
\end{eqnarray}

Multiplying \eqref{eq:solnconst1} by $\left(Z^T D^{-1}Z\right)^{-1}$, (which we note is full rank as long as $N_Y \ge N_{\phi^*}$) we have 
\begin{eqnarray}
\left( Z^T D^{-1} Z \right)^{-1}Z^T D^{-1} Y_{aug} &=& Bd + \left[K + \lambda\left( Z^T D^{-1} Z \right)^{-1}\right] a \label{eq:need_for_d}\\
\left( Z^T D^{-1} Z \right)^{-1}Z^T D^{-1} Y_{aug} &=& Bd + \left[K + \lambda\left( Z^T D^{-1} Z \right)^{-1}\right] Q_2 e \nonumber\\
\Longrightarrow Q_2^T\left( Z^T D^{-1} Z \right)^{-1}Z^T D^{-1} Y_{aug} &=&  Q_2^T\left[K + \lambda\left( Z^T D^{-1} Z \right)^{-1}\right] Q_2 e \nonumber \\
\Longrightarrow e &=&  \left[ Q_2^T\left[K + \lambda\left( Z^T D^{-1} Z \right)^{-1}\right] Q_2 \right]^{-1}Q_2^T\left( Z^T D^{-1} Z \right)^{-1}Z^T D^{-1} Y_{aug} \nonumber\\
\Longrightarrow a &=&  Q_2\left[ Q_2^T\left[K + \lambda\left( Z^T D^{-1} Z \right)^{-1}\right] Q_2 \right]^{-1}Q_2^T\left( Z^T D^{-1} Z \right)^{-1}Z^T D^{-1} Y_{aug} \nonumber
\end{eqnarray}

Solving for $d$ using \eqref{eq:need_for_d} and the $QR$ decomposition of $B$, we obtain
\begin{eqnarray*}
Bd = Q_1R& &= \left( Z^T D^{-1} Z \right)^{-1}Z^T D^{-1} Y_{aug} -  \left[K + \lambda\left( Z^T D^{-1} Z \right)^{-1}\right] a\\
\Longrightarrow d &=& R^{-1}Q_1^T\left[ \left( Z^T D^{-1} Z \right)^{-1}Z^T D^{-1} Y_{aug} -  \left[K + \lambda\left( Z^T D^{-1} Z \right)^{-1}\right] a \right]
\end{eqnarray*}

By the construction of $\mathcal{H}$ as defined in \eqref{eq:HilbertDecomp}, $J\left(\phi^*\right) = \vert \vert {\phi_l^*}^{\prime \prime} \vert \vert^2  + \vert \vert  \phi_m^* \vert \vert^2 + \vert \vert \phi^*_{lm} \vert \vert^2$. The null space of $J\left(\cdot\right)$, $\mathcal{H}_0$, is the set of functions $\lbrace \phi \in \mathcal{H}: J\left(\phi\right) = 0 \rbrace$:

\[
\mathcal{H}_0 = \mathcal{H}_\mu \oplus \mathcal{H}_l^0 = \lbrace 1 \rbrace  \oplus \lbrace k_1 \rbrace
\]
\noindent

	The null space is comprised of functions of the form $\phi^*_0\left(l,m\right) = d_0 + d_1 k_1\left(l\right)$ which obey stationarity in the autoregressive process defined by \eqref{eq:MyModel}, belonging to the class of linear functions of the continuous time lag only. While the solution defined as the minimizer of 


