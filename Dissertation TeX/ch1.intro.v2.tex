\chapter{Introduction}
\label{intro.v2}

%% introduce general covariance problem and why it is important

Nearly all statistical procedures in classical multivariate data analysis, time series analysis, spatial statistics and, more recently, the growing field of statistical learning require an estimate of the covariance matrix or its inverse. Specifically, techniques for clustering and classification such as linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), factor analysis, and principal components analysis (PCA), analysis of conditional independence through graphical models, classical multivariate regression, prediction, and Kriging rely heavily on a covariance estimate, and such an estimate plays a critical role in the performance of the technique. Covariance estimation is an open problem and has been explored extensively in previously work; these hurdles are generally recognized for the difficulty in covariance estimation: the instability of sample covariance matrices in high dimensions, irregularly or sparsely sampled data, and enforcing that covariance estimates are positive definite.

%% introduce general issues with covariance estimation

	%% high dimension
	%% positive definiteness

Prevalent technological advances in industry and many areas of science make high dimensional longitudinal and functional data a common occurrence, arising in numerous areas including medicine and biomedicine, public health, biology, biomechanics and environmental science with specific applications including fMRI, spectroscopic imaging, genetics and gene microarrays among many others, presenting a need for effective covariance estimation in the challenging situation when parameter dimensionality $p$ is possibly much larger than the number of observations, $n$. We consider two types of potentially high dimensional data. The first is the case classical functional data or times series data, where each individual corresponds to a curved sampled densely at a fine grid of longitudinal times, where it is typical that the number of observed time points on any individual is larger than the number of individuals. Alternatively, we may consider sparse longitudinal data where measurement times may be almost completely unique for each individual in the study. In this case, the nature of the high dimensionality may not be a consequence of having more measurements per subject than the number of subjects themselves, but rather because when pooled across subjects, the total number of unique observed time points is greater than the number of individuals. Regularization improves stability of covariance estimates in high dimensions, particularly in the case where the parameter dimensionality $p$ is much larger than the number of observations $n$. 
	
%% How do we hurdle these barriers?

	%% positive definite constraint
To overcome the hurdle of enforcing covariance estimates to be positive definite, several have considered several matrix decompositions including variance-correlation decomposition, Spectral decomposition, and Cholesky decomposition. The Cholesky decomposition has received particular attention, as it which allows for a statistically meaningful interpretation as well as an unconstrained parameterization of elements of the covariance matrix. This parameterization allows for estimation to be accomplished as simply as in least squares regression: if we assume that the data follow an autoregressive process with (possibly) heteroskedastic errors, then the two matrices comprising the Cholesky decomposition, the Cholesky factor (which diagonalizes the covariance matrix) and diagonal matrix itself, hold the autoregressive coefficients and the error variances, respectively. The autoregressive coefficients are often referred to in the literature as the \emph{generalized autoregressive parameters}, or \emph{GARPs}, and the error variances are often called the \emph{innovation variances}, or \emph{IVs}.
	%%regularization

%% What makes us different? 
	%% function estimation vs. estimation of a particular covariance matrix
In longitudinal studies, the measurement schedule could consist of targeted time points or could consist of completely arbitrary (random) time points. If either the measurement schedule has targeted time points which are not necessarily equally spaced or if there is missing data, then we have what is considered incomplete and unbalanced data. If the measurement schedule has arbitrary or almost unique time points for every individual so that at a given time point there could be very few or even only a single measurement, we must consider how to handle what we consider as sparse longitudinal data. We view the response as a stochastic process with corresponding continuous covariance function and the generalized autoregressive parameters as the evaluation of a continuous bivariate function at the pairs of observed time points rather than specifying a finite set of observations to be multivariate normal and estimating the covariance matrix. This is advantageous because it is unlikely that we are only interested in the covariance between pairs of observed design points, so it is reasonable to approach covariance estimation in a way that allows us to obtain an estimate of the covariance between two measurements at any pair of time points within the time interval of interest. 

	%% monotonicity constraint 
%% I am unsure about how much detail to provide here about the mechanics of other methods and how our methods differ from (improve) previous work
This differs from many previous works including that of \citet{bickel2008regularized} and \citet{huang2006covariance} in that they are concern themselves with estimating a specific covariance matrix rather than the parameters of a covariance function. \citet{huang2007estimation}, \citet{kaufman2008covariance}, and \citet{yao2005functional} have viewed the covariance matrix as the evaluation of a smooth function at particular design points. \citet{yao2005functional} do not utilize the Cholesky parameterization, and their estimates are not guaranteed to be positive definite. \citet{kaufman2008covariance} assume a stationary process, restricting covariance estimates to a specific class of functions. They as well as  Huang, Liu, and Liu \cite{huang2007estimation} follow the hueristic argument presented by \cite{pourahmadi1999joint} that $\phi_{t,t-l}$ is monotone decreasing in $l$ and set off-diagonal elements of either the covariance matrix or the Cholesky factor corresponding to large lags to zero. Rather than shrinking element of the Cholesky factor to zero after a particular value of $l$, we choose to softly enforce monotonicity in $l$ by using a hinge penalty as in the work of \citet{tibshirani2011nearly}.
	
The rest of the paper is organized as follows: Section 2 summarizes the general penalized estimation approach in general, and introduces  the transformed coordinates and the penalties for stationarity and non-monotonicity. Section 3 presents a detailed discussion of optimization and computational issues. Section 4 presents a simulation study and a real example to examine the performance of our methods as well as others. Section 5 concludes with discussion and future work.	

