\chapter{Discussion of Results and Future Work}
\label{end.ch}

An innumerable list of statistical techniques depend on a the estimation of a covariance matrix, including Gaussian graphical models, dimensionality reduction methods such as principle components analysis or the singular value decomposition and factor analysis, multivariate regression, geospatial prediction - or Kriging, as well as certain types of clustering and classification. The list of statistical techniques requiring an estimate of a covariance matrix or its inverse is so extensive that it perhaps might be an easier task to enumerate the techniques that don't require one or the other. It is almost unnecessarily verbose to argue that the task of modeling large covariance matrices is an important statistical issue, but the degree of necessity does not indicate the challenge that the task potentially poses, nor does it explain the sources of potential difficulty. 

In practice, particular characteristics of the data generation process can make covariance estimation somewhat of a quandary. Perhaps the most obvious impediment arises when the data are of high dimension. The birth of buzzwords like `Big Data'  is a consequence of recent advances in the automated generation and collection of data, as well as advances in data storage. It is common to now encounter high dimensional data, including very densely-sampled functional data across a multitude of applications. Making use of these data require methodology beyond the traditional techniques, as it well known that the sample covariance matrix exhibits many undesirable properties in high dimensions. The {\bf You need to talk more here about the instability of }


\section{High dimensionality and an expanding parameter space}

In high dimensions, it is well known that the widely-used sample covariance matrix is highly unstable. When we assume the covariance matrix to be unstructured, the number of unknown parameters grows quadratically in the dimensionality of the data, say $p$. This is currently of particular concern, as technological advances have made densely-sampled functional data and even real-time data streaming somewhat of a commonality. 

To address the issue of instability of covariance estimates in high dimensions, many impose structure and reduce the number of free parameters using parsimonious parametric models to characterize the covariance structure in the longitudinal data setting. Simple models depending on a small number of parameters, such as those corresponding to compound symmetry and autoregressive models of some small order, $k$ are commonly found in the literature. While reducing variance of parameter estimates, model misspecification is of potential concern, and we may trade any gain associated with variance reduction for potential bias. Alternatively, several have proposed applying nonparametric methods directly to elements of the sample covariance matrix or a function of the sample covariance matrix. Diggle and Verbyla (1998) introduced a nonparametric estimator obtained by kernel smoothing the sample variogram and squared residuals.  Yao, Mueller, and Wang applied a local linear smoother to the sample covariance matrix in the direction of the diagonal and a local quadratic smoother in the direction orthogonal to the diagonal to account for the presence of additional variation due to measurement error. The estimates yielded by these approaches, however, are not guaranteed to be positive definite. 


\subsection{Constrained estimation}
Additionally, estimates should also be limited to the class of positive-definite structures, potentially leading to a high dimensional constrained optimization problem. 
 
To satisfy the positive-definiteness constraint, methods have been developed and applied to certain reparameterizations of the covariance structure. Chiu, Leonard, and Tsui modeled the matrix logarithm of the covariance matrix. Early nonparametric work using the Spectral decomposition of the covariance matrix included that of Rice and Silverman (1991) which discussed smoothing and smoothing parameter choice for eigenfunction estimation for regularly-spaced data. Staniswalis and Lee (1998) extended kernel-based smoothing of eigenfunctions to functional data observed on irregular grids. However, when the data are sparse in the sense that there are few repeated within-subject measurements and measurement times are quite different from subject-to-subject, approximation of the functional principal component scores defined by the Karhunen-Loeve expansion of the stochastic process by usual integration is unsatisfactory and requires numerical quadrature. Many have explored regression-based approaches using the Spectral decomposition, framing principal components analysis as a least-squares optimization problem. Among many others, Zou, Hastie and Tibshirani (2006), imposed penalties on regression coefficients to induce sparse loadings. 

\subsection{Sparsely or irregularly-sampled data} 

While a wealth of work has been done in the area of time series analysis, it is not uncommon for data to be irregularly sampled, that is, sampling does not occur on an evenly-spaced, regular grid leaving those methods inaccessible. Considering the data from the perspective at the heart of functional data analysis permits optimal use of data which has been irregularly or sparsely sampled.  

Recently, many have considered a modified Cholesky decomposition (MCD) of the inverse of the covariance matrix. This decomposition also ensures positive-definite covariance estimates, and, unlike the Spectral decomposition whose parameters follow an orthogonality constraint, the entries in the MCD of the covariance matrix are unconstrained and have an attractive statistical interpretation as particular regression coefficients and variances.  One drawback we might note, however, is that the interpretation of  the regression model induced by the MCD assumes a natural (time) ordering among the variables in $Y$, which we henceforth assume to have mean $0$, whereas other decompositions are permutation-invariant.

For any positive-definite matrix covariance matrix, $\Sigma$, we have the following unique decomposition:

\begin{equation}
   T^\prime \Sigma T  = D
   \label{eq:MCD}
   \end{equation}
  \noindent 
where $T$ is a lower triangular matrix with diagonal entries all equal to $1$, and $D$ is a diagonal matrix with positive diagonal entries. The lower triangular entries of $T$ are unconstrained, and when the data have a natural ordering (as in time or space), they have a meaningful statistical interpretation. Pourahmadi (1999) proposed modeling

   \begin{equation}   
   \hat{y}_t = \sum_{j=1}^{t-1} \phi_{tj} y_j + \sigma_t \epsilon_t \;\;\;\; t=1,\dots, p 
   \label{eq:Pourahmadimodel} 
   \end{equation}
   \noindent
where $\hat{y}_t$ is the least squares predictor of $y_t$ based on its predecessors, and $\lbrace \epsilon_t =  {y}_t - \hat{y}_t \rbrace$ are uncorrelated with $Cov\left(\epsilon\right) = D = diag\left( \sigma_1^2, \sigma_2^2, \dots, \sigma_p^2 \right)$, $\epsilon = \left(\epsilon_1, \epsilon_2, \dots, \epsilon_p \right)^T$. Then we can write

\[
\epsilon = TY\;\;\;\;\;\;\;\;\;\; T= \left\{ \begin{array}{cr}
-\phi_{ij} & i > j \\
1 & i = j \\
0 & i < j \\
\end{array} \right.
\]
and it follows that  $Cov\left(\epsilon\right)  = T Cov\left( Y \right) T^T = T \Sigma T^T = D$.

We refer to the $\lbrace \phi_{tj} \rbrace$ and the $\lbrace \sigma_t^2 \rbrace$ as the generalized autoregressive parameters (GARPs) and the generalized innovation variances (GIVs), respectively. Several approaches to estimating the covariance parameters have been suggested, the simplest of which is the usual maximum likelihood estimation assuming a Normal distribution for $\epsilon$. Garcia, Kohli, and Pourahmadi (2001) make the heuristic argument that, for fixed $t$, $\phi_{t,t-l}$ should be small for large ``lag'' values and enforce $\phi_{t,t-l}$ to be monotonically decreasing in $l$. To model $\phi$, they examine empirical regressograms: plots of $\hat{\phi}_{t,t-j}$ against lag, $j$ and use these to choose a parametric form for $\hat{\phi}\left(t,t-j\right)$. Similarly they produce innovariograms, plots of the log estimated innovation variances $\hat{\sigma}_t^2$ versus $t$ and select an appropriate functional form, or more simply, the appropriate degree of polynomial. They fit the polynomial model using ordinary least squares. Chen et. al. presented a semiparametric approach for simultaneously estimating a mean function and covariance structure, modeling $\phi\left(s,t\right)$ as a polynomial in $s-t$, inherently making the assumption that the underlying process is stationary and consequently reducing the number of distinct entries in the covariance matrix from $p\left(p-1\right)/2$ to $p$. 

%Stuff about banding the covariance matrix and its relation to antedependence models of order, say, k...
Others attempt to reduce the dimension of the problem by ``banding'' the covariance matrix. Bickel and Levina (2008) directly band the sample covariance matrix $S = \left(\left( s_{ij}\right) \right)$, defining the $k$-banded sample covariance matrix $S_k = $, however, their covariance estimates were not guaranteed to be positive definite. Huang et. al (2006) estimate the elements of $T$ via normal maximum likelihood with a Lasso penalty, which introduces sparsity in $T$ with zeros but in arbitrarily placed entries. Using the Cholesky decomposition to ensure positive-definiteness, Wu and Pourahmadi (2003) presented a $k-diagonal$ estimator which bands the inverse of the covariance matrix by smoothing down the first $k$ diagonals of $T$ and setting the rest to zero, choosing the number of nonzero diagonals by AIC using a normal likelihood. Huang et. al. use spline functions to smooth $\sigma_t^2\rbrace_{t=1}^p$ and $\lbrace \phi_{t,t-j} \rbrace_{j=1}^{t-1}$, the sub-diagonals of $T$ which hold the lag-$J$ regression coefficients and are closely related to time-varying autoregressive models. They use penalized maximum likelihood to estimate the spline coefficients. Treating the $p-1$ sub-diagonals as $p-1$ separate smoothing spline problems does not make use of the potential information about the dependence structure lying in the direction orthogonal to the diagonal; we propose using bivariate smoothing to utilize information in both directions.
 
Rather than a vector of longitudinal data points, view $Y = \left(y_{t_1}, \dots, y_{t_n}\right)^T$ and $\epsilon = \left(\epsilon_{t_1}, \dots, \epsilon_{t_n}\right)^T$, ${t_1} \le \dots \le {t_n}$, as a discretizations of the stochastic processes $y\left(t\right)$ and $e\left(s\right) \sim \mathcal{WN}\left(0,1\right)$.  We view $\phi_{ij}$ and $\sigma_i^2$ as values of the smooth function $\phi\left(s,t\right)$, $0 \le s < t \le 1$ and $S\left(t\right) = \log \sigma^2\left(t \right)$, $0 \le t \le 1$ evaluated at the design points. Consequently, we view the entries of $\Sigma$ as values of a smooth covariance function, $\gamma\left(s,t\right)$, evaluated at the distinct pairs of design points. We define $y$ as a stochastic process as follows:

 \begin{equation}
 y\left(s\right) = \int_0^s \phi\left(s,t\right)y\left(t\right)dt + \sigma\left(s\right)e\left(s\right)
 \end{equation}
 
Re-expressing \eqref{eq:Pourahmadimodel} in terms of $\phi\left(s,t\right)$ and $\sigma^2\left(t\right)$, we have

   \begin{equation}   
   \hat{y}\left(t_i\right)  = \sum_{j=1}^{i-1} \phi\left(t_i ,t_j\right) y\left({t_j}\right) + \sigma\left(t_i\right)\epsilon\left({t_i}\right) \;\;\;\; i=1,\dots, n 
   \label{eq:MyModel} 
   \end{equation}

We first turn our focus to the estimation of $\phi\left(s,t\right)$ and assume that $\sigma^2\left(t\right)$ is known, and WLOG, let $\sigma^2\left(t\right) = 1$ for all $t \in \left[0,1\right]$. It is well known that the sample covariance matrix is unstable when the dimensionality of the data is high, so the attraction of regularization is obvious. Rather than imposing structure on the unconstrained values of $\phi\left(s,t \right)$, we instead consider a rotation of the axes of the input space and re-express $\phi$ in terms of the transformed coordinates $\begin{bmatrix} l\\ m	
\end{bmatrix} = \begin{bmatrix} s - t\\	 \frac{\left(s + t\right)}{2}\end{bmatrix}$

\begin{eqnarray*}
\phi\left(s,t\right) &=& \phi^*\left(s-t,\frac{s+t}{2}\right) \\
 &=& \phi^*\left(l,m\right)\\
\end{eqnarray*}

Imposing structure on $\phi^*$ naturally leads to null models presented in earlier work on covariance estimation with appropriate choice of penalty in the smoothing spline problem formulation. 
    
  
\section{Getting Started}

Well, before you write up your dissertation and zoom over to the
Graduate School with your {\em magnus opus}, be sure to read the Grad
School's Graduate Student Handbook~\cite{osu:guidelines}. It tells you a lot of
stuff you need to know. And since you will be using \LaTeX\ to write
your dissertation, have Lamport's \LaTeXe\ book~\cite{lamport:latex}
handy.

\section{The OSU Dissertation Class}

The latest version of the OSU Dissertation Class (1996) was derived from
the old version originally created in the CIS department.  The new format
specifications are much closer to the way the classes in \LaTeXe\ function
by default; therefore, many of the more challenging aspects of creating the
class are no longer needed.

A lot of graduate students have contributed to the
creation of the dissertation class files {\tt osudissert96.cls} and {\tt
osudissert96-mods.sty}. Elizabeth Zwicky created the original style files. J.
Ramanujam and Con O'Connell are the other main contributors.  Most
recently, Mark Hanes has updated the files to conform to the 1996
Graduate School guidelines.  Al Fencl created
the vita style in {\tt osudissert96}. All other contributors are mentioned
at the top of that file.

This document was initially created by Manas Mandal from Con O'Connell's
dissertation to help a student in Physics.  Since a lot of other people
have expressed interest in the style files and how to use them in a
dissertation, this sample dissertation was produced by Manas Mandal and Al
Fencl.  This document has also been modified by Mark Hanes to reflect the
1996 Graduate School requirements.

\subsection{What you need in order to use {\tt osudissert96}}

To use the {\tt osudissert96} class, you will need to have
%
\begin{enumerate}
\item \LaTeXe\ (obviously).
\item {\tt osudissert96.cls}
\item {\tt osudissert96-mods.sty}
\end{enumerate}
These files are currently all available on the EE HP and Sun Workstations.
%

\subsection{Compiling the Example}
\label{compile.example}

This document is split into several files.  If you have not compiled
it yet or had difficulty compiling it, you should make sure you have
the following files:
%
\begin{center}
\begin{tabular}{l l}
{\tt Thesis.tex} & The main document. Run \LaTeX\ on this file\\
{\tt abstract.tex} & The Abstract for the thesis.\\
{\tt ack.tex} & The Acknowledgement for the thesis.\\
{\tt vita.tex} & The Vita for the author of the thesis.\\
{\tt ch1.intro.tex} & Chapter 1 of the thesis.\\
{\tt ch2.problem.tex} & Chapter 2 of the thesis.\\
{\tt ch3.implem.tex} & Chapter 3 of the thesis.\\
{\tt ch4.end.tex} & Chapter 4 of the thesis.\\
{\tt app1.tex} & The first appendix of the thesis.\\
{\tt app2.tex} & The second appendix of the thesis.\\
{\tt bibfile.bib} & The sample bibliography database.
\end{tabular}
\end{center}
%

To fully compile this example, you should do the following:
%
\begin{enumerate}
\item Run \LaTeX\ on {\tt Thesis}.  This will do the inital
compilation of the document and will create a list of the labels and
references made.
%
\item Run \BibTeX\ on {\tt Thesis}.  This will go into {\tt
bibfile.bib} and extract the appropriate bibliography for the
references  cited in the dissertation.
%
\item Run \LaTeX\ on {\tt Thesis} {\em two} more times.  
The first time, \LaTeX\ will go through and (at the end) will
recognize the references made in the citations and will set up the
table of contents. However, the table of contents will probably be off
since the table of contents will grow.  The second time through,
\LaTeX\ will get the page numbers correct in the table of contents.
\end{enumerate}
%
You will need to perform the above steps on your own
dissertation/thesis as well.

\subsection{Getting more information}

If you need additional information, please check out EE's \LaTeXe\ web
page, {\verb+http://eewww.eng.ohio-state.edu/~hanes/latex2e+}.  If you
can't find what you need there, you might want to read the {\tt .cls} and
{\tt .sty} files used
to generate this dissertation to see how the various commands are used and
start from there. A complete list of the commands defined in {\tt
osudissert96} is also provided in Appendix~\ref{allcommands:app}.

\subsection{Ph.D. Dissertation and Master's Thesis}

The {\tt osudissert96} class provides support for both Ph.D.
dissertations and Master's theses. While this document is an example
of a Ph.D.  dissertation, it is possible generate a Master's thesis
just by including the appropriate documentclass option.  For example, to
produce a Master's of Science thesis, give the option {\tt ms}:

\begin{center}
{\verb+\documentclass[ms]{osudissert96}+}
\end{center}

\section{Organization of this Thesis}

The rest of this thesis is organized as follows. 

Chapter~\ref{prob.ch} will introduce the problems with cows, and what
all has been done by other researchers about it.  In reality,
Chapter~\ref{prob.ch} discusses \LaTeX\ and provides pointers to
advice and examples of how to use the {\tt osudissert96} class.

Chapter~\ref{implem.ch} describes the details of the implementation
method used in having a cow, and how it does solve all the world's
problems. In reality, Chapter~\ref{implem.ch} discusses figures and
tables and how to create them ``easily'' using \LaTeX.

Chapter~\ref{end.ch} summarizes the results of the thesis, and gives
pointers to future research that can be based on this exemplary work.
It has no real bearing on reality.

Appendix~\ref{data.app} explains some of the data used to create
Table~\ref{example-table}. It also has little to do with reality.

Appendix~\ref{allcommands:app} lists all the commands defined in
{\tt osudissert96}.
