%  The dissertation abstract can only be 500 words.



\begin{quote}
In the era of ``Big Data'', the ability to collect and store massive amounts of information has made access to functional and high dimensional data much more prevalent. With this prevalence comes a strong need for methods of estimating large covariance matrices. Sample covariance matrices become extremely unstable estimates of covariance structure as  dimension increases, and the desire to impose a positive-definite constraint on estimates further adds further complexity to the problem. A Cholesky decomposition of the covariance matrix allows for parameter estimation via unconstrained optimization as well as a statistically meaningful interpretation of the parameter estimates. 

Regularization improves stability of covariance estimates in high dimensions, as well as in the case where functional data are sparse and individual curves are sampled at different and possibly un-equally spaced time points. By viewing the entries of the covariance matrix as the evaluation of a continuous bivariate function at the pairs of observed time points, we treat covariance estimation as bivariate smoothing. 

Within regularization framework, we propose novel covariance penalties which are designed to yield natural null models presented in the literature for stationarity or short-term dependence. These penalties are expressed in terms of variation in continuous time lag and its orthogonal complement. In our first approach, we assume that the solution to the optimization problem belongs to the reproducing kernel Hilbert space spanned by the direct sum of the subspace spanned by the first $m$ Bernoulli polynomials and the corresponding orthogonal penalized subspace.

Later, we consider an alternative construction of the function space using a truncated power basis, reflected about the design points. This formulation allows for intuitive specification of the penalty functional to achieve shrinkage toward commonly used models, such as those assuming stationarity, short term dependence, or decaying dependence as the difference between time points increases. We present numerical results and data analysis to illustrate the utility of the proposed method.

\end{quote}
